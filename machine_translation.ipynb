{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "machine_translation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szkics/GerLangReconstruct/blob/main/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlpoGrbJ0udr",
        "outputId": "f3c45681-e8fd-43ba-fe18-d6241b956db2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUGukjV0_ofW"
      },
      "source": [
        "!pip install deep-translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTRBWvZEt2W2",
        "outputId": "17d09653-9cb7-4193-e569-440021ab638d"
      },
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "english_sentence = 'Das Meer ist ein kalter Ort.'\n",
        "german_sentence = GoogleTranslator(source='german', target='dutch').translate(english_sentence, return_all=False)\n",
        "print(german_sentence)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sea is a cold place.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-waNiMR3k8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba56cc98-afba-4c9e-81ee-3e9deb5dcdc6"
      },
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "list_of_german_sentences = []\n",
        "file = open('/content/gdrive/MyDrive/gerlangreconstruct/german_tales.txt','r').read()\n",
        "list_of_german_sentences = tokenize.sent_tokenize(file)\n",
        "print(len(list_of_german_sentences))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "3066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWBEY_V3hFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a091bb5-1d58-46de-ad09-e21d326b4110"
      },
      "source": [
        "import regex as re\n",
        "count = 0\n",
        "german_sent_list = []\n",
        "for sentence in list_of_german_sentences:\n",
        "  processed_sentence = re.sub(r\"\\n\", \" \", sentence)\n",
        "  number_of_words = len(processed_sentence.split())\n",
        "  if (number_of_words <= 16):\n",
        "    count += 1\n",
        "    german_sent_list.append(processed_sentence)\n",
        "count"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1519"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKAeqKrf5ekw"
      },
      "source": [
        "dutch_sent_list = []\n",
        "english_sent_list = []\n",
        "for german_sentence in l_german_sentences[:100]:\n",
        "  english_sentence = GoogleTranslator(source='german', target='english').translate(german_sentence, return_all=False)\n",
        "  english_sent_list.append(english_sentence) \n",
        "  dutch_sentence = GoogleTranslator(source='english', target='dutch').translate(english_sentence, return_all=False)\n",
        "  dutch_sent_list.append(dutch_sentence)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS44l1s27wYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "37b9a73f-706c-48be-d0aa-9e136119c5aa"
      },
      "source": [
        "german_sent_list[13]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'„Koax, Koax, breckekekex,“ war alles, was er sagen konnte, als er das hübsche, kleine Mädchen sah.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZCi5GzM71G4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "130a6278-e545-4495-e5d8-4c0476af1e16"
      },
      "source": [
        "dutch_sent_list[13]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'Coax, coax, breckekekex,' was alles wat hij kon zeggen toen hij het mooie meisje zag.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OAy9nvR8m-7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec6191cd-1faf-4ef4-b7d9-264a9f9f6b49"
      },
      "source": [
        "english_sent_list[13]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"Coax, coax, breckekekex,\" was all he could say when he saw the pretty little girl.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCSXeYwgCJW6"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aee6Ey9SCSgz"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Lower case the sentence\n",
        "    lower_case_sent = sentence.lower()\n",
        "    # Strip punctuation\n",
        "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
        "   \n",
        "    return clean_sentence"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe0C29AiCYD8"
      },
      "source": [
        "def tokenize(sentences):\n",
        "    # Create tokenizer\n",
        "    text_tokenizer = Tokenizer()\n",
        "    # Fit texts\n",
        "    text_tokenizer.fit_on_texts(sentences)\n",
        "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_MxrGD5CdXC"
      },
      "source": [
        "english_sentences = [clean_sentence(tmp) for tmp in english_sent_list]\n",
        "dutch_sentences = [clean_sentence(tmp) for tmp in dutch_sent_list]\n",
        "german_sentences = [clean_sentence(tmp) for tmp in l_german_sentences[:100]]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thXe793iCs6q",
        "outputId": "dc2036da-4fa9-4535-81db-7d59a0b59585"
      },
      "source": [
        "# Tokenize words\n",
        "english_text_tokenized, english_text_tokenizer = tokenize(english_sentences)\n",
        "german_text_tokenized, german_text_tokenizer = tokenize(german_sentences)\n",
        "dutch_text_tokenized, dutch_text_tokenizer = tokenize(dutch_sentences)\n",
        "\n",
        "print(english_text_tokenized)\n",
        "print(english_text_tokenizer)\n",
        "\n",
        "# Check language length\n",
        "english_vocab = len(english_text_tokenizer.word_index) + 1\n",
        "print(\"English vocabulary is of {} unique words\".format(english_vocab))\n",
        "\n",
        "german_vocab = len(german_text_tokenizer.word_index) + 1\n",
        "print(\"german vocabulary is of {} unique words\".format(german_vocab))\n",
        "\n",
        "dutch_vocab = len(dutch_text_tokenizer.word_index) + 1\n",
        "print(\"dutch vocabulary is of {} unique words\".format(dutch_vocab))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[152, 45, 1, 153, 87, 29, 154, 155], [156, 87], [88, 89, 90, 91, 92, 90, 91, 157, 93], [158, 29, 93, 159, 15, 92], [160, 161, 162], [163, 164], [94, 95, 9, 165, 21, 166, 6, 1, 167], [4, 5, 35, 96, 97, 10, 168, 169, 59, 170, 4, 5, 60, 11], [7, 171, 15, 4, 24, 98, 19, 172, 20, 1, 173, 174, 1, 46], [7, 36, 61, 99, 175, 176, 100, 177], [9, 101, 178, 62], [7, 30, 179, 180, 94, 31, 181, 2, 182, 102, 25, 36, 63, 183, 184], [185, 12, 5, 103, 31, 104, 2, 186, 1, 64, 187, 16, 65, 47], [105, 105, 188, 5, 13, 12, 30, 189, 48, 12, 190, 1, 191, 26, 106], [32, 7, 192, 22, 1, 104, 193, 37, 3, 1, 194, 195, 11, 5, 196], [1, 197, 107, 20, 1, 198, 2, 4, 107, 199, 200, 201], [202, 66, 203, 108, 11, 5, 48, 1, 204, 33, 38, 22, 17, 38, 1, 109], [205, 12, 206, 110, 111, 12, 67, 14, 3, 207, 3, 208], [18, 33, 112, 22, 17, 45, 1, 109, 2, 209, 17, 20, 10, 210], [211, 1, 113, 11, 212, 13, 213, 15, 1, 114, 115], [31, 113, 2, 49, 214, 19, 50, 27, 51, 1, 68, 116, 51], [32, 7, 27, 3, 1, 52, 215, 216], [69, 64, 217, 218, 16, 10, 53, 219, 117, 1, 220, 221], [222, 223, 14, 224, 225, 6, 1, 52, 70], [226, 227, 228, 3, 229, 71, 118, 46], [12, 27, 2, 230, 65, 231, 15, 65, 232, 233, 234], [12, 5, 54, 235, 2, 54, 236, 6, 1, 52, 70], [19, 12, 119, 17, 55, 3, 21, 237, 29, 1, 238, 39, 9, 5, 239, 15, 240], [1, 108, 39, 120, 14, 241, 16, 1, 68], [4, 120, 21, 242, 3, 21, 243, 10, 26, 39], [121, 122, 10, 39, 14, 100, 13, 123, 40, 48, 51, 124], [19, 11, 30, 55, 125, 24, 98], [9, 5, 1, 244, 245], [246, 8, 54, 247, 8, 62, 26, 248, 6, 1, 249, 28, 3, 17], [72, 126, 127, 250, 38], [24, 73, 7, 36, 251, 3, 252, 2, 34, 253, 67, 254, 3, 17, 7, 128, 55, 129], [35, 130, 255, 6, 11], [256, 74], [8, 257, 258, 106, 259, 1, 28, 2, 33, 37, 75, 1, 131], [40, 40, 132, 1, 39, 2, 33, 75, 1, 133, 115], [11, 5, 54, 260], [7, 63, 134, 261, 3, 262, 37, 15, 1, 56, 131], [19, 7, 263, 24, 13, 264, 135, 7, 5, 29, 35, 265, 266, 16, 1, 136, 76], [48, 4, 5, 49, 11, 36, 13, 17, 267, 268], [8, 77, 3, 14, 78, 269, 15, 270, 271, 6, 1, 52, 70, 3, 17], [19, 11, 272, 2, 6, 7, 273, 41, 1, 136, 76], [1, 76, 36, 274, 137, 3, 110, 11], [74, 74, 7, 6, 275, 17, 276, 277, 10, 53, 278, 79, 9, 42, 279, 3, 4], [7, 101, 38, 4, 5, 1, 28, 9, 5, 103, 280, 29], [138, 124, 1, 68, 281, 6, 1, 28, 72, 126, 282, 111, 3, 1, 56, 80], [139, 8, 41, 3, 283, 71], [8, 284, 285, 20, 140, 81], [24, 73, 18, 27, 3, 1, 56, 80], [286, 2, 287, 288, 15, 1, 289, 290, 2, 291, 292, 293, 294, 22, 69, 295], [19, 1, 28, 57, 33, 20, 2, 4, 134, 296, 82, 2, 82], [45, 297, 298, 10, 99, 299, 300, 42, 117, 1, 301, 302, 133, 303, 29, 1, 304, 305], [141, 59, 140, 142, 6, 1, 28], [1, 28, 33, 112, 22, 306, 2, 143, 4, 20, 25, 16, 1, 307, 308], [12, 5, 35, 96, 97, 11, 309], [4, 5, 1, 310, 16, 311], [18, 144, 312, 3, 313, 81, 2, 50, 7, 145, 30, 83, 45, 79, 3, 79], [7, 132, 17, 40, 40, 3, 4], [84, 14, 1, 64, 314, 16, 9], [1, 43], [89, 88, 315, 85, 5, 10, 43, 146, 20, 1, 73, 142, 16, 10, 53, 316], [10, 26, 317, 16, 4, 1, 23, 147, 42, 318, 2, 319, 20, 1, 320, 321], [25, 67, 14, 322, 9, 4, 5, 323, 37, 16, 324, 12, 42, 31, 57], [2, 12, 42, 325, 20, 25, 326], [327, 328, 34, 1, 58, 77, 329, 6, 1, 26, 23, 44], [19, 1, 58, 330, 331, 4, 332, 2, 333, 24, 1, 23, 22, 69, 334], [335, 84, 21, 336, 2, 337, 119, 1, 23, 44], [35, 338, 55, 339, 1, 47], [340, 341, 342, 3, 83, 343, 344, 8], [32, 4, 345, 37, 3, 1, 346, 2, 347, 1, 86], [4, 348, 21, 10, 349, 148, 2, 32, 34, 350, 1, 23, 44], [351, 13, 1, 43, 9, 352, 141, 15, 1, 353, 354, 2, 32, 1, 114, 49, 149, 355], [8, 356, 15, 1, 357, 2, 150, 86, 148, 358, 359, 13, 1, 58], [360, 123, 127, 82, 85], [13, 46, 116, 25, 122, 361, 19, 150], [1, 23, 147, 27, 118, 46, 22, 362, 86, 26, 363, 2, 13, 1, 23, 364, 9, 30, 21, 365], [66, 18, 366], [50, 367, 24, 71, 6, 1, 47], [368, 8, 14, 3, 369, 78, 370], [31, 8, 14, 3, 143, 78, 371], [25, 61], [25, 61], [372], [85, 18, 373, 135, 18, 144, 57, 145, 374], [72, 375, 41, 3, 376, 6, 10, 377, 2, 378, 81, 75, 1, 146], [139, 8, 41, 1, 58, 3, 137, 2, 379, 8, 380, 8, 2, 381, 8], [382, 383, 384, 34, 18, 41], [34, 128, 18, 129, 385, 66, 386, 44, 387, 2, 53, 30, 21], [50, 27, 49], [13, 1, 43, 388, 389, 3, 83, 3, 1, 56, 80, 390, 51], [34, 391, 149, 9, 392, 4], [138, 84, 14, 3, 393, 394, 18, 6], [95, 6, 1, 47, 23], [121, 130, 27, 38, 22, 59, 395, 1, 396, 397], [1, 62, 26, 44, 125, 2, 151, 31, 398, 102, 18, 63, 151, 399], [400, 32, 13, 43, 14, 401, 60, 402, 2, 77, 57, 60, 9, 403]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7f5ac257d190>\n",
            "English vocabulary is of 404 unique words\n",
            "german vocabulary is of 475 unique words\n",
            "dutch vocabulary is of 411 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGz0pMI-C677"
      },
      "source": [
        "max_sentence_length = 16\n",
        "english_pad_sentence = pad_sequences(english_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "german_pad_sentence = pad_sequences(german_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "dutch_pad_sentence = pad_sequences(dutch_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "\n",
        "# Reshape data\n",
        "english_pad_sentence = english_pad_sentence.reshape(*english_pad_sentence.shape, 1)\n",
        "german_pad_sentence = german_pad_sentence.reshape(*german_pad_sentence.shape, 1)\n",
        "dutch_pad_sentence = dutch_pad_sentence.reshape(*dutch_pad_sentence.shape, 1)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6PHOFDFzpn"
      },
      "source": [
        "input_shape = (max_sentence_length, 2)\n",
        "input_sequence = Input(input_shape, name='InputLayer')\n",
        "rnn = LSTM(512, return_sequences=True, dropout=0.1, name='RNNLayer')(input_sequence)\n",
        "logits = TimeDistributed(Dense(german_vocab), name='TimeDistributed')(rnn)\n",
        "\n",
        "model = Model(input_sequence, Activation('softmax')(logits))\n",
        "model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(1e-2),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMseV64gGCk4",
        "outputId": "d6639581-afd8-4e11-9192-a9546e89b707"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "InputLayer (InputLayer)      [(None, 16, 2)]           0         \n",
            "_________________________________________________________________\n",
            "RNNLayer (LSTM)              (None, 16, 512)           1054720   \n",
            "_________________________________________________________________\n",
            "TimeDistributed (TimeDistrib (None, 16, 475)           243675    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 475)           0         \n",
            "=================================================================\n",
            "Total params: 1,298,395\n",
            "Trainable params: 1,298,395\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDNDsiseGDwD"
      },
      "source": [
        "print(english_pad_sentence[0].shape)\n",
        "print(english_pad_sentence[0])\n",
        "\n",
        "print(dutch_pad_sentence[0].shape)\n",
        "print(dutch_pad_sentence[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAa8LEzKGOLe"
      },
      "source": [
        "english_dutch_data = np.array([np.concatenate((english_pad_sentence[i], dutch_pad_sentence[i]), axis=1) for i in range(len(english_pad_sentence))])\n",
        "print(english_dutch_data.shape)\n",
        "print(english_dutch_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzr541xEGY8z",
        "outputId": "fabc8579-467a-4fd9-9010-4f650db147d4"
      },
      "source": [
        "model_results = model.fit(english_dutch_data, german_pad_sentence, batch_size=10, epochs=200)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 3s 127ms/step - loss: 4.7297 - accuracy: 0.3034\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 3.8947 - accuracy: 0.3409\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 3.5371 - accuracy: 0.3751\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 3.3659 - accuracy: 0.3800\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 3.0184 - accuracy: 0.4316\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 1s 125ms/step - loss: 2.8193 - accuracy: 0.4503\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 1s 122ms/step - loss: 2.8672 - accuracy: 0.4213\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 1s 123ms/step - loss: 2.6096 - accuracy: 0.4587\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 2.4483 - accuracy: 0.4840\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 2.4272 - accuracy: 0.4762\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 2.2368 - accuracy: 0.5021\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 2.2477 - accuracy: 0.5098\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 2.0189 - accuracy: 0.5493\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 1.9688 - accuracy: 0.5661\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 1.7521 - accuracy: 0.6072\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 1.7907 - accuracy: 0.6256\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 1.6275 - accuracy: 0.6297\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 1.5940 - accuracy: 0.6500\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 1.6477 - accuracy: 0.6291\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 1.3649 - accuracy: 0.7132\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 1.3696 - accuracy: 0.7107\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 1.4058 - accuracy: 0.6946\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 1.3157 - accuracy: 0.7148\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 1.2507 - accuracy: 0.7390\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 1s 125ms/step - loss: 1.2161 - accuracy: 0.7038\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 1.0706 - accuracy: 0.7575\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.9580 - accuracy: 0.7685\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 1.0583 - accuracy: 0.7617\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 1.1228 - accuracy: 0.7388\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.9471 - accuracy: 0.7715\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 1.0771 - accuracy: 0.7601\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.9520 - accuracy: 0.7888\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 1.0799 - accuracy: 0.7470\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 1.0840 - accuracy: 0.7440\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.9145 - accuracy: 0.7846\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.9350 - accuracy: 0.7871\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.9344 - accuracy: 0.7906\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.8601 - accuracy: 0.7902\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.8949 - accuracy: 0.7823\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.8737 - accuracy: 0.7925\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.8340 - accuracy: 0.7981\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.8025 - accuracy: 0.8108\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.8547 - accuracy: 0.7784\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.6624 - accuracy: 0.8373\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.7536 - accuracy: 0.8138\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.8331 - accuracy: 0.7867\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.7616 - accuracy: 0.8103\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.7212 - accuracy: 0.8254\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.7293 - accuracy: 0.8246\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.6301 - accuracy: 0.8294\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.6284 - accuracy: 0.8425\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.6334 - accuracy: 0.8324\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.7391 - accuracy: 0.8170\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.7685 - accuracy: 0.8059\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.6982 - accuracy: 0.8318\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.7078 - accuracy: 0.8272\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.6236 - accuracy: 0.8375\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.5688 - accuracy: 0.8447\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.6899 - accuracy: 0.8266\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.6234 - accuracy: 0.8332\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.7579 - accuracy: 0.8062\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.6752 - accuracy: 0.8359\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.6334 - accuracy: 0.8458\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.5838 - accuracy: 0.8415\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.5387 - accuracy: 0.8680\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.5701 - accuracy: 0.8459\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.6459 - accuracy: 0.8261\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.6653 - accuracy: 0.8213\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.5789 - accuracy: 0.8405\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 1s 124ms/step - loss: 0.6042 - accuracy: 0.8331\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.5683 - accuracy: 0.8448\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.5963 - accuracy: 0.8504\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 1s 124ms/step - loss: 0.5325 - accuracy: 0.8417\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.5068 - accuracy: 0.8725\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 1s 123ms/step - loss: 0.5498 - accuracy: 0.8566\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.5588 - accuracy: 0.8533\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.4926 - accuracy: 0.8777\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 1s 124ms/step - loss: 0.4896 - accuracy: 0.8827\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.5787 - accuracy: 0.8567\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.5008 - accuracy: 0.8750\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4661 - accuracy: 0.8807\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.4549 - accuracy: 0.8745\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4672 - accuracy: 0.8700\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.5523 - accuracy: 0.8466\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.4666 - accuracy: 0.8795\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.5366 - accuracy: 0.8576\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4209 - accuracy: 0.8802\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.5445 - accuracy: 0.8597\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4552 - accuracy: 0.8777\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 1s 141ms/step - loss: 0.4534 - accuracy: 0.8816\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.5128 - accuracy: 0.8705\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.5080 - accuracy: 0.8654\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4880 - accuracy: 0.8745\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.4822 - accuracy: 0.8758\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4572 - accuracy: 0.8606\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.4857 - accuracy: 0.8732\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4467 - accuracy: 0.8732\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4870 - accuracy: 0.8573\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4724 - accuracy: 0.8832\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4493 - accuracy: 0.8848\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.4816 - accuracy: 0.8769\n",
            "Epoch 102/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4689 - accuracy: 0.8846\n",
            "Epoch 103/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.5009 - accuracy: 0.8709\n",
            "Epoch 104/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.5885 - accuracy: 0.8456\n",
            "Epoch 105/200\n",
            "10/10 [==============================] - 1s 140ms/step - loss: 0.5117 - accuracy: 0.8674\n",
            "Epoch 106/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.5288 - accuracy: 0.8708\n",
            "Epoch 107/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4840 - accuracy: 0.8811\n",
            "Epoch 108/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.4466 - accuracy: 0.8717\n",
            "Epoch 109/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.4365 - accuracy: 0.8791\n",
            "Epoch 110/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.5106 - accuracy: 0.8466\n",
            "Epoch 111/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.4428 - accuracy: 0.8822\n",
            "Epoch 112/200\n",
            "10/10 [==============================] - 1s 125ms/step - loss: 0.4163 - accuracy: 0.8846\n",
            "Epoch 113/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4296 - accuracy: 0.8878\n",
            "Epoch 114/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.3431 - accuracy: 0.9028\n",
            "Epoch 115/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4969 - accuracy: 0.8630\n",
            "Epoch 116/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.4908 - accuracy: 0.8652\n",
            "Epoch 117/200\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.4923 - accuracy: 0.8625\n",
            "Epoch 118/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.3641 - accuracy: 0.8975\n",
            "Epoch 119/200\n",
            "10/10 [==============================] - 1s 145ms/step - loss: 0.4378 - accuracy: 0.8810\n",
            "Epoch 120/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.3055 - accuracy: 0.9116\n",
            "Epoch 121/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4798 - accuracy: 0.8828\n",
            "Epoch 122/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.4098 - accuracy: 0.8887\n",
            "Epoch 123/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4659 - accuracy: 0.8778\n",
            "Epoch 124/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4014 - accuracy: 0.8884\n",
            "Epoch 125/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4618 - accuracy: 0.8760\n",
            "Epoch 126/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.4242 - accuracy: 0.8862\n",
            "Epoch 127/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4070 - accuracy: 0.8971\n",
            "Epoch 128/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4368 - accuracy: 0.8855\n",
            "Epoch 129/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.4757 - accuracy: 0.8705\n",
            "Epoch 130/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.3874 - accuracy: 0.8930\n",
            "Epoch 131/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.4120 - accuracy: 0.8941\n",
            "Epoch 132/200\n",
            "10/10 [==============================] - 1s 141ms/step - loss: 0.4247 - accuracy: 0.8909\n",
            "Epoch 133/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.4164 - accuracy: 0.8857\n",
            "Epoch 134/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.3441 - accuracy: 0.9062\n",
            "Epoch 135/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.5126 - accuracy: 0.8584\n",
            "Epoch 136/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.4971 - accuracy: 0.8725\n",
            "Epoch 137/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.5454 - accuracy: 0.8518\n",
            "Epoch 138/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4076 - accuracy: 0.8863\n",
            "Epoch 139/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.4154 - accuracy: 0.8845\n",
            "Epoch 140/200\n",
            "10/10 [==============================] - 1s 141ms/step - loss: 0.4945 - accuracy: 0.8671\n",
            "Epoch 141/200\n",
            "10/10 [==============================] - 1s 137ms/step - loss: 0.4452 - accuracy: 0.8759\n",
            "Epoch 142/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.5034 - accuracy: 0.8695\n",
            "Epoch 143/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4875 - accuracy: 0.8687\n",
            "Epoch 144/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4083 - accuracy: 0.8843\n",
            "Epoch 145/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.4348 - accuracy: 0.8812\n",
            "Epoch 146/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4203 - accuracy: 0.8798\n",
            "Epoch 147/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3165 - accuracy: 0.9040\n",
            "Epoch 148/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4434 - accuracy: 0.8809\n",
            "Epoch 149/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3953 - accuracy: 0.8971\n",
            "Epoch 150/200\n",
            "10/10 [==============================] - 1s 149ms/step - loss: 0.4376 - accuracy: 0.8759\n",
            "Epoch 151/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.3308 - accuracy: 0.9073\n",
            "Epoch 152/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.4086 - accuracy: 0.8885\n",
            "Epoch 153/200\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 0.3906 - accuracy: 0.8883\n",
            "Epoch 154/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.3805 - accuracy: 0.8944\n",
            "Epoch 155/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.4207 - accuracy: 0.8746\n",
            "Epoch 156/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.5091 - accuracy: 0.8654\n",
            "Epoch 157/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.4355 - accuracy: 0.8862\n",
            "Epoch 158/200\n",
            "10/10 [==============================] - 1s 144ms/step - loss: 0.4180 - accuracy: 0.8882\n",
            "Epoch 159/200\n",
            "10/10 [==============================] - 1s 142ms/step - loss: 0.3531 - accuracy: 0.9056\n",
            "Epoch 160/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3982 - accuracy: 0.8891\n",
            "Epoch 161/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.3705 - accuracy: 0.8974\n",
            "Epoch 162/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.4810 - accuracy: 0.8721\n",
            "Epoch 163/200\n",
            "10/10 [==============================] - 1s 124ms/step - loss: 0.3989 - accuracy: 0.8797\n",
            "Epoch 164/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4602 - accuracy: 0.8764\n",
            "Epoch 165/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.5355 - accuracy: 0.8644\n",
            "Epoch 166/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3753 - accuracy: 0.8913\n",
            "Epoch 167/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.4410 - accuracy: 0.8743\n",
            "Epoch 168/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.4967 - accuracy: 0.8841\n",
            "Epoch 169/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3570 - accuracy: 0.8913\n",
            "Epoch 170/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.3514 - accuracy: 0.9006\n",
            "Epoch 171/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4045 - accuracy: 0.8770\n",
            "Epoch 172/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.4031 - accuracy: 0.8888\n",
            "Epoch 173/200\n",
            "10/10 [==============================] - 1s 128ms/step - loss: 0.3178 - accuracy: 0.9086\n",
            "Epoch 174/200\n",
            "10/10 [==============================] - 1s 126ms/step - loss: 0.4863 - accuracy: 0.8628\n",
            "Epoch 175/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4940 - accuracy: 0.8738\n",
            "Epoch 176/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.3744 - accuracy: 0.8839\n",
            "Epoch 177/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.3291 - accuracy: 0.8963\n",
            "Epoch 178/200\n",
            "10/10 [==============================] - 1s 147ms/step - loss: 0.3381 - accuracy: 0.9067\n",
            "Epoch 179/200\n",
            "10/10 [==============================] - 2s 150ms/step - loss: 0.3790 - accuracy: 0.8984\n",
            "Epoch 180/200\n",
            "10/10 [==============================] - 1s 140ms/step - loss: 0.4223 - accuracy: 0.8786\n",
            "Epoch 181/200\n",
            "10/10 [==============================] - 2s 151ms/step - loss: 0.3792 - accuracy: 0.8858\n",
            "Epoch 182/200\n",
            "10/10 [==============================] - 1s 146ms/step - loss: 0.3919 - accuracy: 0.8893\n",
            "Epoch 183/200\n",
            "10/10 [==============================] - 2s 153ms/step - loss: 0.3330 - accuracy: 0.9016\n",
            "Epoch 184/200\n",
            "10/10 [==============================] - 1s 143ms/step - loss: 0.3807 - accuracy: 0.8899\n",
            "Epoch 185/200\n",
            "10/10 [==============================] - 1s 139ms/step - loss: 0.3842 - accuracy: 0.9000\n",
            "Epoch 186/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.3727 - accuracy: 0.8908\n",
            "Epoch 187/200\n",
            "10/10 [==============================] - 1s 132ms/step - loss: 0.3582 - accuracy: 0.9002\n",
            "Epoch 188/200\n",
            "10/10 [==============================] - 1s 134ms/step - loss: 0.3376 - accuracy: 0.9032\n",
            "Epoch 189/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.3250 - accuracy: 0.9054\n",
            "Epoch 190/200\n",
            "10/10 [==============================] - 1s 130ms/step - loss: 0.3495 - accuracy: 0.9021\n",
            "Epoch 191/200\n",
            "10/10 [==============================] - 1s 135ms/step - loss: 0.3633 - accuracy: 0.8935\n",
            "Epoch 192/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.3638 - accuracy: 0.9041\n",
            "Epoch 193/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.3653 - accuracy: 0.9029\n",
            "Epoch 194/200\n",
            "10/10 [==============================] - 1s 138ms/step - loss: 0.4869 - accuracy: 0.8710\n",
            "Epoch 195/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.3558 - accuracy: 0.8951\n",
            "Epoch 196/200\n",
            "10/10 [==============================] - 1s 136ms/step - loss: 0.3452 - accuracy: 0.9003\n",
            "Epoch 197/200\n",
            "10/10 [==============================] - 1s 129ms/step - loss: 0.4115 - accuracy: 0.8864\n",
            "Epoch 198/200\n",
            "10/10 [==============================] - 1s 133ms/step - loss: 0.3236 - accuracy: 0.9132\n",
            "Epoch 199/200\n",
            "10/10 [==============================] - 1s 131ms/step - loss: 0.4762 - accuracy: 0.8671\n",
            "Epoch 200/200\n",
            "10/10 [==============================] - 1s 143ms/step - loss: 0.4005 - accuracy: 0.8910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWRiwzufHckQ"
      },
      "source": [
        "def logits_to_sentence(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<empty>' \n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u51sF2aqHg0i",
        "outputId": "0ad3ecf8-18c3-46be-d644-1afd2cc81d76"
      },
      "source": [
        "index = 60\n",
        "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
        "print(\"The dutch sentence is: {}\".format(dutch_sentences[index]))\n",
        "# AFTER 1 EPOCH\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The english sentence is: they were fastened to thumbelies back and now she too could fly from flower to flower\n",
            "The dutch sentence is: ze werden vastgemaakt aan duimelijntjes rug en nu kon ook zij van bloem naar bloem vliegen\n",
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: <empty> die nicht die nicht nicht <empty> nicht nicht nicht die als nicht als er nicht\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6UUIl-AIr9t",
        "outputId": "f1093558-f7da-452d-c7cc-82f368045025"
      },
      "source": [
        "# AFTER 10 EPOCHS\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sie däumelieschen sie däumelieschen däumelieschen däumelieschen und sagte die <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftp7UlMnJN0W",
        "outputId": "74022137-91ca-4698-98df-08aeb87bb9bb"
      },
      "source": [
        "# AFTER 100 EPOCHS\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: sie blickte rücken größer rücken niedlich und auf er auch sie sie wiesen der konnte nie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfT7f_KoNs55",
        "outputId": "d1210b75-ec9b-4747-daea-076ecaec5aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# AFTER 100 EPOCHS AND CHANGING NUMBER OF HIDDEN NODES TO 512 AND DROPOUT TO 0.2\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: sie wurden rücken am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji_kvuwTPSHW",
        "outputId": "e86ace77-f41a-4da4-ccec-0de3f00058b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# AFTER 100 EPOCHS AND CHANGING DROPOUT TO 0.1 (BEST MODEL YET FOR THIS DATA, 512 + 0.1)\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmRViHftSQgX",
        "outputId": "39601f36-bc01-455a-a8ad-9c6939d873b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 100 EPOCHS, 1024, 0.1\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YfA6AFITwWo",
        "outputId": "1b319123-5e14-4f77-c3b2-b514929a3a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 200 EPOCHS, 512, 0.1\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5afe395560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLbfDuPzUGl5"
      },
      "source": [
        "english_test_sentence = clean_sentence(\"The little girl was lost in the forest and suddenly a wolf appeared.\")\n",
        "dutch_test_sentence = clean_sentence(\"Het kleine meisje was verdwaald in het bos en plotseling verscheen er een wolf.\")\n",
        "expected_german_sentence = clean_sentence(\"Das kleine Mädchen verirrte sich im Wald und plötzlich tauchte ein Wolf auf.\")"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSGBpCABVae0"
      },
      "source": [
        "english_sentences.append(english_test_sentence)\n",
        "dutch_sentences.append(dutch_test_sentence)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHqknpE5WgRC"
      },
      "source": [
        "english_text_tokenized, english_text_tokenizer = tokenize(english_sentences)\n",
        "dutch_text_tokenized, dutch_text_tokenizer = tokenize(dutch_sentences)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgQvmFu0XR2Z"
      },
      "source": [
        "english_pad_sentence = english_pad_sentence.reshape(*english_pad_sentence.shape, 1)\n",
        "dutch_pad_sentence = dutch_pad_sentence.reshape(*dutch_pad_sentence.shape, 1)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746Fqyv5Xd-e"
      },
      "source": [
        "english_dutch_data = np.array([np.concatenate((english_pad_sentence[i], dutch_pad_sentence[i]), axis=1) for i in range(len(english_pad_sentence))])\n",
        "print(english_dutch_data.shape)\n",
        "print(english_dutch_data[99])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RTDvBY2Xy4b",
        "outputId": "e4056d3d-d578-42fa-f8c1-c3eae01e1760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "index = 99\n",
        "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
        "print(\"The dutch sentence is: {}\".format(dutch_sentences[index]))\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The english sentence is: since then all storks have been called peter and are still called that today\n",
            "The dutch sentence is: sindsdien heten alle ooievaars peter en worden ze nu nog steeds zo genoemd\n",
            "The actual german sentence is    : seitdem hießen alle störche peter und werden noch heute so genannt\n",
            "The predicted german sentence is: „laßt hießen alle störche peter und werden noch heute so genannt <empty> <empty> <empty> <empty> <empty>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}