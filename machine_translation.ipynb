{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "machine_translation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szkics/GerLangReconstruct/blob/main/machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlpoGrbJ0udr",
        "outputId": "f3c45681-e8fd-43ba-fe18-d6241b956db2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUGukjV0_ofW"
      },
      "source": [
        "!pip install deep-translator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTRBWvZEt2W2",
        "outputId": "17d09653-9cb7-4193-e569-440021ab638d"
      },
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "english_sentence = 'Das Meer ist ein kalter Ort.'\n",
        "german_sentence = GoogleTranslator(source='german', target='dutch').translate(english_sentence, return_all=False)\n",
        "print(german_sentence)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sea is a cold place.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-waNiMR3k8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba56cc98-afba-4c9e-81ee-3e9deb5dcdc6"
      },
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "list_of_german_sentences = []\n",
        "file = open('/content/gdrive/MyDrive/gerlangreconstruct/german_tales.txt','r').read()\n",
        "list_of_german_sentences = tokenize.sent_tokenize(file)\n",
        "print(len(list_of_german_sentences))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "3066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZWBEY_V3hFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a091bb5-1d58-46de-ad09-e21d326b4110"
      },
      "source": [
        "import regex as re\n",
        "count = 0\n",
        "german_sent_list = []\n",
        "for sentence in list_of_german_sentences:\n",
        "  processed_sentence = re.sub(r\"\\n\", \" \", sentence)\n",
        "  number_of_words = len(processed_sentence.split())\n",
        "  if (number_of_words <= 16):\n",
        "    count += 1\n",
        "    german_sent_list.append(processed_sentence)\n",
        "count"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1519"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKAeqKrf5ekw"
      },
      "source": [
        "dutch_sent_list = []\n",
        "english_sent_list = []\n",
        "for german_sentence in l_german_sentences[:100]:\n",
        "  english_sentence = GoogleTranslator(source='german', target='english').translate(german_sentence, return_all=False)\n",
        "  english_sent_list.append(english_sentence) \n",
        "  dutch_sentence = GoogleTranslator(source='english', target='dutch').translate(english_sentence, return_all=False)\n",
        "  dutch_sent_list.append(dutch_sentence)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS44l1s27wYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "37b9a73f-706c-48be-d0aa-9e136119c5aa"
      },
      "source": [
        "german_sent_list[13]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'„Koax, Koax, breckekekex,“ war alles, was er sagen konnte, als er das hübsche, kleine Mädchen sah.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZCi5GzM71G4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "130a6278-e545-4495-e5d8-4c0476af1e16"
      },
      "source": [
        "dutch_sent_list[13]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'Coax, coax, breckekekex,' was alles wat hij kon zeggen toen hij het mooie meisje zag.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OAy9nvR8m-7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec6191cd-1faf-4ef4-b7d9-264a9f9f6b49"
      },
      "source": [
        "english_sent_list[13]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"Coax, coax, breckekekex,\" was all he could say when he saw the pretty little girl.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCSXeYwgCJW6"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aee6Ey9SCSgz"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "    # Lower case the sentence\n",
        "    lower_case_sent = sentence.lower()\n",
        "    # Strip punctuation\n",
        "    string_punctuation = string.punctuation + \"¡\" + '¿'\n",
        "    clean_sentence = lower_case_sent.translate(str.maketrans('', '', string_punctuation))\n",
        "   \n",
        "    return clean_sentence"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe0C29AiCYD8"
      },
      "source": [
        "def tokenize(sentences):\n",
        "    # Create tokenizer\n",
        "    text_tokenizer = Tokenizer()\n",
        "    # Fit texts\n",
        "    text_tokenizer.fit_on_texts(sentences)\n",
        "    return text_tokenizer.texts_to_sequences(sentences), text_tokenizer"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_MxrGD5CdXC"
      },
      "source": [
        "english_sentences = [clean_sentence(tmp) for tmp in english_sent_list]\n",
        "dutch_sentences = [clean_sentence(tmp) for tmp in dutch_sent_list]\n",
        "german_sentences = [clean_sentence(tmp) for tmp in l_german_sentences[:100]]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thXe793iCs6q",
        "outputId": "dc2036da-4fa9-4535-81db-7d59a0b59585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Tokenize words\n",
        "english_text_tokenized, english_text_tokenizer = tokenize(english_sentences)\n",
        "german_text_tokenized, german_text_tokenizer = tokenize(german_sentences)\n",
        "dutch_text_tokenized, dutch_text_tokenizer = tokenize(dutch_sentences)\n",
        "\n",
        "print(english_text_tokenized)\n",
        "print(english_text_tokenizer)\n",
        "\n",
        "# Check language length\n",
        "english_vocab = len(english_text_tokenizer.word_index) + 1\n",
        "print(\"English vocabulary is of {} unique words\".format(english_vocab))\n",
        "\n",
        "german_vocab = len(german_text_tokenizer.word_index) + 1\n",
        "print(\"german vocabulary is of {} unique words\".format(german_vocab))\n",
        "\n",
        "dutch_vocab = len(dutch_text_tokenizer.word_index) + 1\n",
        "print(\"dutch vocabulary is of {} unique words\".format(dutch_vocab))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[152, 45, 1, 153, 87, 29, 154, 155], [156, 87], [88, 89, 90, 91, 92, 90, 91, 157, 93], [158, 29, 93, 159, 15, 92], [160, 161, 162], [163, 164], [94, 95, 9, 165, 21, 166, 6, 1, 167], [4, 5, 35, 96, 97, 10, 168, 169, 59, 170, 4, 5, 60, 11], [7, 171, 15, 4, 24, 98, 19, 172, 20, 1, 173, 174, 1, 46], [7, 36, 61, 99, 175, 176, 100, 177], [9, 101, 178, 62], [7, 30, 179, 180, 94, 31, 181, 2, 182, 102, 25, 36, 63, 183, 184], [185, 12, 5, 103, 31, 104, 2, 186, 1, 64, 187, 16, 65, 47], [105, 105, 188, 5, 13, 12, 30, 189, 48, 12, 190, 1, 191, 26, 106], [32, 7, 192, 22, 1, 104, 193, 37, 3, 1, 194, 195, 11, 5, 196], [1, 197, 107, 20, 1, 198, 2, 4, 107, 199, 200, 201], [202, 66, 203, 108, 11, 5, 48, 1, 204, 33, 38, 22, 17, 38, 1, 109], [205, 12, 206, 110, 111, 12, 67, 14, 3, 207, 3, 208], [18, 33, 112, 22, 17, 45, 1, 109, 2, 209, 17, 20, 10, 210], [211, 1, 113, 11, 212, 13, 213, 15, 1, 114, 115], [31, 113, 2, 49, 214, 19, 50, 27, 51, 1, 68, 116, 51], [32, 7, 27, 3, 1, 52, 215, 216], [69, 64, 217, 218, 16, 10, 53, 219, 117, 1, 220, 221], [222, 223, 14, 224, 225, 6, 1, 52, 70], [226, 227, 228, 3, 229, 71, 118, 46], [12, 27, 2, 230, 65, 231, 15, 65, 232, 233, 234], [12, 5, 54, 235, 2, 54, 236, 6, 1, 52, 70], [19, 12, 119, 17, 55, 3, 21, 237, 29, 1, 238, 39, 9, 5, 239, 15, 240], [1, 108, 39, 120, 14, 241, 16, 1, 68], [4, 120, 21, 242, 3, 21, 243, 10, 26, 39], [121, 122, 10, 39, 14, 100, 13, 123, 40, 48, 51, 124], [19, 11, 30, 55, 125, 24, 98], [9, 5, 1, 244, 245], [246, 8, 54, 247, 8, 62, 26, 248, 6, 1, 249, 28, 3, 17], [72, 126, 127, 250, 38], [24, 73, 7, 36, 251, 3, 252, 2, 34, 253, 67, 254, 3, 17, 7, 128, 55, 129], [35, 130, 255, 6, 11], [256, 74], [8, 257, 258, 106, 259, 1, 28, 2, 33, 37, 75, 1, 131], [40, 40, 132, 1, 39, 2, 33, 75, 1, 133, 115], [11, 5, 54, 260], [7, 63, 134, 261, 3, 262, 37, 15, 1, 56, 131], [19, 7, 263, 24, 13, 264, 135, 7, 5, 29, 35, 265, 266, 16, 1, 136, 76], [48, 4, 5, 49, 11, 36, 13, 17, 267, 268], [8, 77, 3, 14, 78, 269, 15, 270, 271, 6, 1, 52, 70, 3, 17], [19, 11, 272, 2, 6, 7, 273, 41, 1, 136, 76], [1, 76, 36, 274, 137, 3, 110, 11], [74, 74, 7, 6, 275, 17, 276, 277, 10, 53, 278, 79, 9, 42, 279, 3, 4], [7, 101, 38, 4, 5, 1, 28, 9, 5, 103, 280, 29], [138, 124, 1, 68, 281, 6, 1, 28, 72, 126, 282, 111, 3, 1, 56, 80], [139, 8, 41, 3, 283, 71], [8, 284, 285, 20, 140, 81], [24, 73, 18, 27, 3, 1, 56, 80], [286, 2, 287, 288, 15, 1, 289, 290, 2, 291, 292, 293, 294, 22, 69, 295], [19, 1, 28, 57, 33, 20, 2, 4, 134, 296, 82, 2, 82], [45, 297, 298, 10, 99, 299, 300, 42, 117, 1, 301, 302, 133, 303, 29, 1, 304, 305], [141, 59, 140, 142, 6, 1, 28], [1, 28, 33, 112, 22, 306, 2, 143, 4, 20, 25, 16, 1, 307, 308], [12, 5, 35, 96, 97, 11, 309], [4, 5, 1, 310, 16, 311], [18, 144, 312, 3, 313, 81, 2, 50, 7, 145, 30, 83, 45, 79, 3, 79], [7, 132, 17, 40, 40, 3, 4], [84, 14, 1, 64, 314, 16, 9], [1, 43], [89, 88, 315, 85, 5, 10, 43, 146, 20, 1, 73, 142, 16, 10, 53, 316], [10, 26, 317, 16, 4, 1, 23, 147, 42, 318, 2, 319, 20, 1, 320, 321], [25, 67, 14, 322, 9, 4, 5, 323, 37, 16, 324, 12, 42, 31, 57], [2, 12, 42, 325, 20, 25, 326], [327, 328, 34, 1, 58, 77, 329, 6, 1, 26, 23, 44], [19, 1, 58, 330, 331, 4, 332, 2, 333, 24, 1, 23, 22, 69, 334], [335, 84, 21, 336, 2, 337, 119, 1, 23, 44], [35, 338, 55, 339, 1, 47], [340, 341, 342, 3, 83, 343, 344, 8], [32, 4, 345, 37, 3, 1, 346, 2, 347, 1, 86], [4, 348, 21, 10, 349, 148, 2, 32, 34, 350, 1, 23, 44], [351, 13, 1, 43, 9, 352, 141, 15, 1, 353, 354, 2, 32, 1, 114, 49, 149, 355], [8, 356, 15, 1, 357, 2, 150, 86, 148, 358, 359, 13, 1, 58], [360, 123, 127, 82, 85], [13, 46, 116, 25, 122, 361, 19, 150], [1, 23, 147, 27, 118, 46, 22, 362, 86, 26, 363, 2, 13, 1, 23, 364, 9, 30, 21, 365], [66, 18, 366], [50, 367, 24, 71, 6, 1, 47], [368, 8, 14, 3, 369, 78, 370], [31, 8, 14, 3, 143, 78, 371], [25, 61], [25, 61], [372], [85, 18, 373, 135, 18, 144, 57, 145, 374], [72, 375, 41, 3, 376, 6, 10, 377, 2, 378, 81, 75, 1, 146], [139, 8, 41, 1, 58, 3, 137, 2, 379, 8, 380, 8, 2, 381, 8], [382, 383, 384, 34, 18, 41], [34, 128, 18, 129, 385, 66, 386, 44, 387, 2, 53, 30, 21], [50, 27, 49], [13, 1, 43, 388, 389, 3, 83, 3, 1, 56, 80, 390, 51], [34, 391, 149, 9, 392, 4], [138, 84, 14, 3, 393, 394, 18, 6], [95, 6, 1, 47, 23], [121, 130, 27, 38, 22, 59, 395, 1, 396, 397], [1, 62, 26, 44, 125, 2, 151, 31, 398, 102, 18, 63, 151, 399], [400, 32, 13, 43, 14, 401, 60, 402, 2, 77, 57, 60, 9, 403]]\n",
            "<keras_preprocessing.text.Tokenizer object at 0x7f5ac257d190>\n",
            "English vocabulary is of 404 unique words\n",
            "german vocabulary is of 475 unique words\n",
            "dutch vocabulary is of 411 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGz0pMI-C677"
      },
      "source": [
        "max_sentence_length = 16\n",
        "english_pad_sentence = pad_sequences(english_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "german_pad_sentence = pad_sequences(german_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "dutch_pad_sentence = pad_sequences(dutch_text_tokenized, max_sentence_length, padding = \"post\")\n",
        "\n",
        "# Reshape data\n",
        "english_pad_sentence = english_pad_sentence.reshape(*english_pad_sentence.shape, 1)\n",
        "german_pad_sentence = german_pad_sentence.reshape(*german_pad_sentence.shape, 1)\n",
        "dutch_pad_sentence = dutch_pad_sentence.reshape(*dutch_pad_sentence.shape, 1)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6PHOFDFzpn"
      },
      "source": [
        "input_shape = (max_sentence_length, 2)\n",
        "input_sequence = Input(input_shape, name='InputLayer')\n",
        "rnn = LSTM(256, return_sequences=True, dropout=0.5, name='RNNLayer')(input_sequence)\n",
        "logits = TimeDistributed(Dense(german_vocab), name='TimeDistributed')(rnn)\n",
        "\n",
        "model = Model(input_sequence, Activation('softmax')(logits))\n",
        "model.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(1e-2),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMseV64gGCk4",
        "outputId": "a1dc1e7b-8246-4626-af84-69aaa33e7673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "InputLayer (InputLayer)      [(None, 16, 2)]           0         \n",
            "_________________________________________________________________\n",
            "RNNLayer (LSTM)              (None, 16, 256)           265216    \n",
            "_________________________________________________________________\n",
            "TimeDistributed (TimeDistrib (None, 16, 475)           122075    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 475)           0         \n",
            "=================================================================\n",
            "Total params: 387,291\n",
            "Trainable params: 387,291\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDNDsiseGDwD"
      },
      "source": [
        "print(english_pad_sentence[0].shape)\n",
        "print(english_pad_sentence[0])\n",
        "\n",
        "print(dutch_pad_sentence[0].shape)\n",
        "print(dutch_pad_sentence[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAa8LEzKGOLe"
      },
      "source": [
        "english_dutch_data = np.array([np.concatenate((english_pad_sentence[i], dutch_pad_sentence[i]), axis=1) for i in range(len(english_pad_sentence))])\n",
        "print(english_dutch_data.shape)\n",
        "print(english_dutch_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzr541xEGY8z",
        "outputId": "c40d3415-6309-4beb-92f3-69d86ca2f2e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_results = model.fit(english_dutch_data, german_pad_sentence, batch_size=10, epochs=100)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.3138 - accuracy: 0.4187\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.2851 - accuracy: 0.4206\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.3028 - accuracy: 0.4212\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 3.2364 - accuracy: 0.4294\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.2622 - accuracy: 0.4269\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.1919 - accuracy: 0.4288\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 3.1516 - accuracy: 0.4269\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 3.1511 - accuracy: 0.4269\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 3.1420 - accuracy: 0.4306\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 3.0857 - accuracy: 0.4238\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.0610 - accuracy: 0.4294\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 3.0415 - accuracy: 0.4275\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 3.0233 - accuracy: 0.4294\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 3.0589 - accuracy: 0.4300\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 2.9751 - accuracy: 0.4319\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 3.0128 - accuracy: 0.4319\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.9370 - accuracy: 0.4300\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.9098 - accuracy: 0.4275\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.8905 - accuracy: 0.4269\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.9065 - accuracy: 0.4275\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.8975 - accuracy: 0.4294\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.8584 - accuracy: 0.4313\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.8139 - accuracy: 0.4381\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 2.8379 - accuracy: 0.4400\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.7626 - accuracy: 0.4469\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 2.7929 - accuracy: 0.4344\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.8011 - accuracy: 0.4375\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.8132 - accuracy: 0.4263\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.7687 - accuracy: 0.4375\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.8052 - accuracy: 0.4462\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 2.7290 - accuracy: 0.4412\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.6796 - accuracy: 0.4462\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 2.7314 - accuracy: 0.4400\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 39ms/step - loss: 2.7026 - accuracy: 0.4487\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.6748 - accuracy: 0.4456\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.6745 - accuracy: 0.4431\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.6532 - accuracy: 0.4387\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.6085 - accuracy: 0.4525\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.6278 - accuracy: 0.4581\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.5965 - accuracy: 0.4588\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.6508 - accuracy: 0.4538\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.6254 - accuracy: 0.4569\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.6579 - accuracy: 0.4419\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.5983 - accuracy: 0.4500\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.5801 - accuracy: 0.4506\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.6075 - accuracy: 0.4444\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.5052 - accuracy: 0.4575\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 49ms/step - loss: 2.5352 - accuracy: 0.4631\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 48ms/step - loss: 2.5952 - accuracy: 0.4550\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.6153 - accuracy: 0.4469\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.5687 - accuracy: 0.4500\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.4795 - accuracy: 0.4619\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.4971 - accuracy: 0.4600\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.5149 - accuracy: 0.4544\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.4985 - accuracy: 0.4588\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.4960 - accuracy: 0.4638\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.4771 - accuracy: 0.4613\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.4568 - accuracy: 0.4650\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.4308 - accuracy: 0.4688\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.4507 - accuracy: 0.4613\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.4132 - accuracy: 0.4669\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 38ms/step - loss: 2.4162 - accuracy: 0.4681\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.3843 - accuracy: 0.4688\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.4723 - accuracy: 0.4550\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.3168 - accuracy: 0.4750\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.3918 - accuracy: 0.4700\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.3204 - accuracy: 0.4850\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.3887 - accuracy: 0.4712\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.3461 - accuracy: 0.4737\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.3115 - accuracy: 0.4750\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.3548 - accuracy: 0.4675\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.3754 - accuracy: 0.4675\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.3684 - accuracy: 0.4762\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.3381 - accuracy: 0.4794\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.3710 - accuracy: 0.4731\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.3297 - accuracy: 0.4800\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.3030 - accuracy: 0.4875\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.3669 - accuracy: 0.4712\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.2900 - accuracy: 0.4844\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.2388 - accuracy: 0.4850\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.2289 - accuracy: 0.4869\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.2498 - accuracy: 0.4800\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 45ms/step - loss: 2.2748 - accuracy: 0.4794\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.2542 - accuracy: 0.4888\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 47ms/step - loss: 2.2515 - accuracy: 0.4850\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.2834 - accuracy: 0.4737\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.2712 - accuracy: 0.4794\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 46ms/step - loss: 2.3138 - accuracy: 0.4706\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.2720 - accuracy: 0.4906\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.2645 - accuracy: 0.4875\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.2650 - accuracy: 0.4906\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.1556 - accuracy: 0.5006\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.2751 - accuracy: 0.4806\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 40ms/step - loss: 2.2358 - accuracy: 0.4812\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 41ms/step - loss: 2.1984 - accuracy: 0.4956\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.1779 - accuracy: 0.4994\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 42ms/step - loss: 2.2273 - accuracy: 0.4925\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 43ms/step - loss: 2.2491 - accuracy: 0.4956\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.2325 - accuracy: 0.4844\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 44ms/step - loss: 2.2074 - accuracy: 0.4925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWRiwzufHckQ"
      },
      "source": [
        "def logits_to_sentence(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {idx: word for word, idx in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = '<empty>' \n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u51sF2aqHg0i",
        "outputId": "0ad3ecf8-18c3-46be-d644-1afd2cc81d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "index = 60\n",
        "print(\"The english sentence is: {}\".format(english_sentences[index]))\n",
        "print(\"The dutch sentence is: {}\".format(dutch_sentences[index]))\n",
        "# AFTER 1 EPOCH\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The english sentence is: they were fastened to thumbelies back and now she too could fly from flower to flower\n",
            "The dutch sentence is: ze werden vastgemaakt aan duimelijntjes rug en nu kon ook zij van bloem naar bloem vliegen\n",
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: <empty> die nicht die nicht nicht <empty> nicht nicht nicht die als nicht als er nicht\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6UUIl-AIr9t",
        "outputId": "f1093558-f7da-452d-c7cc-82f368045025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# AFTER 10 EPOCHS\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sie däumelieschen sie däumelieschen däumelieschen däumelieschen und sagte die <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftp7UlMnJN0W",
        "outputId": "74022137-91ca-4698-98df-08aeb87bb9bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# AFTER 100 EPOCHS\n",
        "print(\"The actual german sentence is    : {}\".format(german_sentences[index]))\n",
        "print('The predicted german sentence is: ', end=\"\")\n",
        "print(logits_to_sentence(model.predict(english_dutch_data[index:index+1])[0], german_text_tokenizer))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The actual german sentence is    : sie wurden däumelieschen am rücken befestigt und nun konnte auch sie von blume zu blume fliegen\n",
            "The predicted german sentence is: sie blickte rücken größer rücken niedlich und auf er auch sie sie wiesen der konnte nie\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}